"""
PERMANENT FIX: Complete Database Manager Replacement
Replace your ENTIRE src/database/database_manager.py with this file
Handles ALL Japan auction formats including your BMW Excel permanently
"""

import sqlite3
import pandas as pd
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import os
import re

from config.settings import Settings

class DatabaseManager:
    """Permanently fixed database manager that handles ALL Japan auction data formats"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.settings = Settings()
        self.db_path = self.settings.DATABASE_PATH
        
        # Ensure database directory exists
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        self.logger.info(f"Database manager initialized with path: {self.db_path}")
    
    def initialize_database(self):
        """Initialize database with complete schema"""
        self.logger.info("Initializing database tables")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # UK market data table
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS uk_market_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    item_id TEXT UNIQUE,
                    title TEXT,
                    make TEXT,
                    model TEXT,
                    year INTEGER,
                    price REAL,
                    currency TEXT,
                    mileage REAL,
                    fuel_type TEXT,
                    transmission TEXT,
                    body_type TEXT,
                    condition TEXT,
                    url TEXT,
                    image_url TEXT,
                    seller TEXT,
                    location TEXT,
                    shipping_cost REAL,
                    age INTEGER,
                    source TEXT,
                    collection_timestamp TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                ''')
                
                # Japan auction data table - Complete schema for all formats
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS japan_auction_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    auction_id TEXT UNIQUE,
                    title TEXT,
                    make TEXT,
                    model TEXT,
                    year INTEGER,
                    mileage_km REAL,
                    mileage_miles REAL,
                    final_price_jpy REAL,
                    final_price_gbp REAL,
                    auction_house TEXT,
                    auction_date TEXT,
                    grade TEXT,
                    grade_score REAL,
                    fuel_type TEXT,
                    transmission TEXT,
                    body_type TEXT,
                    colour TEXT,
                    steering TEXT,
                    drive_type TEXT,
                    seats INTEGER,
                    doors INTEGER,
                    engine_details TEXT,
                    estimated_import_cost REAL,
                    shipping_cost REAL,
                    import_duty REAL,
                    vat REAL,
                    other_costs REAL,
                    total_landed_cost REAL,
                    image_url TEXT,
                    promo_badges TEXT,
                    stock_reference TEXT,
                    source TEXT,
                    collection_timestamp TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                ''')
                
                # Government data table
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS government_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    data_type TEXT,
                    data_content TEXT,
                    collection_timestamp TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                ''')
                
                # Analysis results table
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    make TEXT,
                    model TEXT,
                    year INTEGER,
                    year_range TEXT,
                    uk_sample_size INTEGER,
                    japan_sample_size INTEGER,
                    match_type TEXT,
                    uk_avg_price REAL,
                    uk_median_price REAL,
                    uk_min_price REAL,
                    uk_max_price REAL,
                    uk_price_std REAL,
                    uk_avg_mileage REAL,
                    uk_median_mileage REAL,
                    uk_listings_count INTEGER,
                    japan_avg_auction_price REAL,
                    japan_median_auction_price REAL,
                    japan_min_auction_price REAL,
                    japan_max_auction_price REAL,
                    japan_avg_import_cost REAL,
                    japan_avg_total_cost REAL,
                    japan_median_total_cost REAL,
                    japan_avg_mileage REAL,
                    japan_avg_grade REAL,
                    japan_auctions_count INTEGER,
                    gross_profit REAL,
                    profit_margin REAL,
                    profit_margin_conservative REAL,
                    roi REAL,
                    price_volatility_uk REAL,
                    risk_score REAL,
                    market_share REAL,
                    listings_density INTEGER,
                    price_trend TEXT,
                    demand_score REAL,
                    avg_days_listed REAL,
                    profitability_score REAL,
                    market_demand_score REAL,
                    risk_assessment_score REAL,
                    liquidity_score REAL,
                    market_trends_score REAL,
                    final_score REAL,
                    final_score_percentile REAL,
                    score_grade TEXT,
                    investment_category TEXT,
                    recommendation TEXT,
                    overall_score REAL,
                    analysis_timestamp TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                ''')
                
                # Create indexes for better performance
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_uk_make_model_year ON uk_market_data(make, model, year)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_japan_make_model_year ON japan_auction_data(make, model, year)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_analysis_score ON analysis_results(final_score DESC)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_collection_timestamp ON uk_market_data(collection_timestamp)')
                
                conn.commit()
                self.logger.info("Database initialization completed")
                
        except Exception as e:
            self.logger.error(f"Error initializing database: {str(e)}")
            raise
    
    def extract_make_model_from_title(self, title: str) -> tuple:
        """Extract make and model from car title"""
        if not title or pd.isna(title):
            return None, None
        
        title = str(title).strip()
        
        # Car makes to look for (comprehensive list)
        car_makes = [
            'Toyota', 'Honda', 'Nissan', 'Mazda', 'Subaru', 'Mitsubishi', 'Suzuki', 'Isuzu', 'Daihatsu',
            'Lexus', 'Infiniti', 'Acura', 'BMW', 'Mercedes', 'Audi', 'Volkswagen', 'Porsche', 'Mini',
            'Ford', 'Chevrolet', 'Chrysler', 'Jeep', 'Hyundai', 'Kia', 'Volvo', 'Peugeot', 'Renault',
            'Fiat', 'Alfa Romeo', 'Ferrari', 'Lamborghini', 'Maserati', 'Jaguar', 'Land Rover', 'Bentley',
            'Rolls-Royce', 'McLaren', 'Aston Martin'
        ]
        
        # Try to find make in title
        make = None
        for car_make in car_makes:
            if car_make.lower() in title.lower():
                make = car_make
                break
        
        # Extract model (words after make, before year/numbers)
        model = None
        if make:
            # Remove make from title and extract model
            title_after_make = title.lower().replace(make.lower(), '').strip()
            
            # Split by common separators and take first meaningful word(s)
            words = re.split(r'[\s\-_]+', title_after_make)
            model_words = []
            
            for word in words:
                if word and not re.match(r'^\d{4}$', word):  # Skip years
                    if not re.match(r'^\d+(\.\d+)?[lL]?$', word):  # Skip engine sizes
                        model_words.append(word.title())
                        if len(model_words) >= 2:  # Take max 2 words for model
                            break
            
            if model_words:
                model = ' '.join(model_words)
        
        return make, model
    
    def extract_year_from_text(self, year_text):
        """Extract year from various text formats like '2018/11', '2018', etc."""
        if pd.isna(year_text):
            return None
        
        year_str = str(year_text)
        # Extract first 4 digits that start with 20 or 19
        match = re.search(r'\b((?:19|20)\d{2})\b', year_str)
        if match:
            year = int(match.group(1))
            # Validate year range
            current_year = datetime.now().year
            if 1990 <= year <= current_year + 1:
                return year
        return None
    
    def extract_mileage_from_text(self, mileage_text):
        """Extract mileage from text like '13777 km', '45,000 miles', etc."""
        if pd.isna(mileage_text):
            return None
        
        mileage_str = str(mileage_text).replace(',', '')
        # Extract numbers
        match = re.search(r'(\d+)', mileage_str)
        if match:
            mileage = int(match.group(1))
            # Convert if it's likely in miles (indicated by 'mile' or if value is very low)
            if 'mile' in mileage_str.lower() or (mileage < 200000 and 'km' not in mileage_str.lower()):
                return mileage / 0.621371  # Convert miles to km
            return mileage
        return None
    
    def extract_price_from_text(self, price_text):
        """Extract price from text like '$10,976', 'Â¥2,200,000', 'Â£15,000', etc."""
        if pd.isna(price_text) or str(price_text).strip() in ['-', '', 'nan']:
            return 0.0, 'USD'
        
        price_str = str(price_text)
        
        # Detect currency
        currency = 'USD'  # Default
        if 'Â¥' in price_str or 'JPY' in price_str.upper():
            currency = 'JPY'
        elif 'Â£' in price_str or 'GBP' in price_str.upper():
            currency = 'GBP'
        elif '$' in price_str or 'USD' in price_str.upper():
            currency = 'USD'
        elif 'â‚¬' in price_str or 'EUR' in price_str.upper():
            currency = 'EUR'
        
        # Extract number
        cleaned = re.sub(r'[^\d,.]', '', price_str)
        cleaned = cleaned.replace(',', '')
        
        try:
            return float(cleaned), currency
        except:
            return 0.0, currency
    
    def clean_transmission(self, trans_text):
        """Clean transmission text to standard format"""
        if pd.isna(trans_text):
            return None
        
        trans_str = str(trans_text).upper().strip()
        
        # Mapping for common transmission codes
        transmission_map = {
            'AT': 'Automatic',
            'AUTO': 'Automatic',
            'AUTOMATIC': 'Automatic',
            'MT': 'Manual',
            'MANUAL': 'Manual',
            'CVT': 'CVT',
            'AMT': 'Automated Manual',
            'DCT': 'Dual Clutch',
            '4AT': 'Automatic',
            '5AT': 'Automatic',
            '6AT': 'Automatic',
            '5MT': 'Manual',
            '6MT': 'Manual'
        }
        
        return transmission_map.get(trans_str, trans_str.title())
    
    def convert_currency_to_gbp(self, amount: float, from_currency: str) -> float:
        """Convert currency to GBP using current rates"""
        if amount == 0:
            return 0.0
        
        # Exchange rates (approximate)
        rates = {
            'GBP': 1.0,
            'USD': 0.75,
            'JPY': 0.0055,
            'EUR': 0.85
        }
        
        rate = rates.get(from_currency, 0.75)  # Default to USD rate
        return amount * rate
    
    def map_japan_columns_universal(self, df: pd.DataFrame) -> pd.DataFrame:
        """Universal column mapping for ALL Japan auction data formats"""
        
        # Comprehensive column mapping dictionary
        column_mappings = {
            # ID fields
            'auction_id': [
                'Car Stock ID', 'Stock ID', 'stock_id', 'auction_id', 'lot_number', 'lot_no', 
                'id', 'vehicle_id', 'item_id', 'reference', 'ref'
            ],
            
            # Title/Description
            'title': [
                'Car Title', 'title', 'description', 'vehicle_name', 'name', 'vehicle_description',
                'car_name', 'vehicle_title', 'full_title', 'listing_title'
            ],
            
            # Year
            'year': [
                'Car Year', 'Year', 'year', 'model_year', 'manufacture_year', 'production_year',
                'registration_year', 'yr', 'vehicle_year'
            ],
            
            # Mileage
            'mileage_km': [
                'Car Mileage', 'Mileage', 'mileage', 'odometer', 'km', 'kilometers', 'distance',
                'mileage_km', 'odometer_km', 'kms', 'mileage_reading'
            ],
            
            # Transmission
            'transmission': [
                'Car Transmission', 'Transmission', 'transmission', 'gearbox', 'trans', 'gear',
                'transmission_type', 'gearbox_type'
            ],
            
            # Model/Code
            'model': [
                'Car Model Code', 'Model Code', 'model_code', 'model', 'model_name', 'vehicle_model',
                'car_model', 'model_variant', 'variant'
            ],
            
            # Engine/Fuel
            'fuel_type': [
                'Car Fuel', 'Fuel', 'fuel_type', 'Car Engine', 'Engine', 'engine_type', 'fuel',
                'power_source', 'engine_fuel', 'propulsion'
            ],
            
            'engine_details': [
                'Engine', 'engine_size', 'engine_capacity', 'displacement', 'cc', 'liters',
                'engine_details', 'motor'
            ],
            
            # Color
            'colour': [
                'Car Color', 'Color', 'colour', 'color', 'paint', 'exterior_color', 'car_color',
                'paint_color', 'body_color'
            ],
            
            # Physical attributes
            'steering': [
                'Car Steering', 'Steering', 'steering', 'steering_type', 'hand_drive',
                'steering_position'
            ],
            
            'drive_type': [
                'Car Drive Type', 'Drive Type', 'drive_type', 'drivetrain', 'drive', 'wheel_drive',
                'traction'
            ],
            
            'seats': [
                'Car Seats', 'Seats', 'seats', 'seating', 'seat_count', 'seating_capacity',
                'passengers'
            ],
            
            'doors': [
                'Car Doors', 'Doors', 'doors', 'door_count', 'num_doors'
            ],
            
            # Location/Auction
            'auction_house': [
                'Car Location', 'Location', 'location', 'auction_house', 'auction', 'venue',
                'auction_venue', 'sale_location', 'branch', 'site'
            ],
            
            # Pricing
            'final_price_gbp': [
                'Car Price', 'Price', 'price', 'final_price', 'sale_price', 'hammer_price',
                'winning_bid', 'final_price_gbp', 'gbp_price', 'price_gbp'
            ],
            
            'final_price_jpy': [
                'price_jpy', 'jpy_price', 'yen_price', 'auction_price_jpy', 'final_price_jpy'
            ],
            
            'total_landed_cost': [
                'Car Total Price', 'Total Price', 'total_price', 'total_cost', 'landed_cost',
                'final_cost', 'all_in_price', 'delivered_price'
            ],
            
            # Media/Extra
            'image_url': [
                'Car Image URL', 'Image URL', 'image_url', 'photo_url', 'picture_url', 'image',
                'photo', 'thumbnail', 'main_image'
            ],
            
            'promo_badges': [
                'Promo Badges', 'badges', 'promotions', 'features', 'highlights', 'tags',
                'special_features', 'selling_points'
            ],
            
            # Auction specific
            'grade': [
                'grade', 'condition_grade', 'exterior_grade', 'rating', 'condition_rating',
                'auction_grade', 'quality_grade'
            ],
            
            'grade_score': [
                'grade_score', 'score', 'numeric_grade', 'condition_score', 'quality_score'
            ],
            
            'auction_date': [
                'auction_date', 'date', 'sale_date', 'auction_time', 'sale_time', 'sold_date'
            ],
            
            # Import costs
            'estimated_import_cost': [
                'estimated_import_cost', 'import_cost', 'import_fee', 'import_charges'
            ],
            
            'shipping_cost': [
                'shipping_cost', 'shipping', 'freight', 'transport_cost', 'delivery_cost'
            ],
            
            'import_duty': [
                'import_duty', 'duty', 'customs_duty', 'customs_fee', 'tariff'
            ],
            
            'vat': [
                'vat', 'tax', 'value_added_tax', 'sales_tax'
            ],
            
            'other_costs': [
                'other_costs', 'misc_costs', 'additional_costs', 'extra_fees', 'other_fees'
            ]
        }
        
        # Create new DataFrame with mapped columns
        mapped_df = pd.DataFrame()
        
        self.logger.info(f"Universal mapping input columns: {list(df.columns)}")
        
        # Map each expected column
        mapping_log = []
        for target_col, possible_cols in column_mappings.items():
            found = False
            for possible_col in possible_cols:
                # Check for exact match (case insensitive)
                matching_cols = [col for col in df.columns if col.lower() == possible_col.lower()]
                if matching_cols:
                    mapped_df[target_col] = df[matching_cols[0]]
                    mapping_log.append(f"Exact: {matching_cols[0]} -> {target_col}")
                    found = True
                    break
                
                # Check for partial match
                matching_cols = [col for col in df.columns if possible_col.lower() in col.lower()]
                if matching_cols:
                    mapped_df[target_col] = df[matching_cols[0]]
                    mapping_log.append(f"Partial: {matching_cols[0]} -> {target_col}")
                    found = True
                    break
            
            # Set defaults for missing columns
            if not found:
                if target_col in ['final_price_jpy', 'final_price_gbp', 'total_landed_cost']:
                    mapped_df[target_col] = 0.0
                elif target_col == 'grade_score':
                    mapped_df[target_col] = 6.0  # Good default grade
                elif target_col == 'collection_timestamp':
                    mapped_df[target_col] = datetime.now().isoformat()
                elif target_col == 'source':
                    mapped_df[target_col] = 'universal_upload'
                else:
                    mapped_df[target_col] = None
        
        # Log successful mappings
        for log_entry in mapping_log:
            self.logger.info(log_entry)
        
        # Process and clean the mapped data
        mapped_df = self.clean_mapped_data(mapped_df)
        
        self.logger.info(f"Universal mapping completed. Output columns: {list(mapped_df.columns)}")
        
        return mapped_df
    
    def clean_mapped_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and process mapped data"""
        
        if df.empty:
            return df
        
        try:
            # Clean year data
            if 'year' in df.columns:
                df['year'] = df['year'].apply(self.extract_year_from_text)
            
            # Clean mileage data
            if 'mileage_km' in df.columns:
                df['mileage_km'] = df['mileage_km'].apply(self.extract_mileage_from_text)
                df['mileage_miles'] = df['mileage_km'] * 0.621371
            
            # Clean transmission data
            if 'transmission' in df.columns:
                df['transmission'] = df['transmission'].apply(self.clean_transmission)
            
            # Extract make and model from title if not present
            if 'title' in df.columns and df['title'].notna().any():
                if 'make' not in df.columns or df['make'].isna().all():
                    makes_models = df['title'].apply(self.extract_make_model_from_title)
                    df['make'] = [mm[0] for mm in makes_models]
                    if 'model' not in df.columns or df['model'].isna().all():
                        df['model'] = [mm[1] for mm in makes_models]
            
            # Handle price conversion
            if 'final_price_gbp' in df.columns:
                price_data = df['final_price_gbp'].apply(self.extract_price_from_text)
                prices = [pd[0] for pd in price_data]
                currencies = [pd[1] for pd in price_data]
                
                # Convert all prices to GBP
                df['final_price_gbp'] = [
                    self.convert_currency_to_gbp(price, currency) 
                    for price, currency in zip(prices, currencies)
                ]
                
                # Convert to JPY
                df['final_price_jpy'] = df['final_price_gbp'] / 0.0055
            
            # Calculate total landed cost if missing
            if 'total_landed_cost' in df.columns:
                df['total_landed_cost'] = df['total_landed_cost'].fillna(
                    df['final_price_gbp'] * 1.35  # Add 35% for import costs
                )
            
            # Clean numeric fields
            numeric_fields = ['year', 'mileage_km', 'final_price_gbp', 'final_price_jpy', 'total_landed_cost', 'seats', 'doors']
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors='coerce')
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error cleaning mapped data: {e}")
            return df
    
    def store_uk_data(self, uk_data: pd.DataFrame):
        """Store UK market data"""
        if uk_data.empty:
            self.logger.warning("No UK data to store")
            return
        
        try:
            expected_columns = [
                'item_id', 'title', 'make', 'model', 'year', 'price', 'currency',
                'mileage', 'fuel_type', 'transmission', 'body_type', 'condition',
                'url', 'image_url', 'seller', 'location', 'shipping_cost', 'age',
                'source', 'collection_timestamp'
            ]
            
            # Ensure all expected columns exist
            uk_clean = uk_data.copy()
            for col in expected_columns:
                if col not in uk_clean.columns:
                    if col in ['price', 'year', 'mileage', 'shipping_cost', 'age']:
                        uk_clean[col] = 0
                    elif col == 'currency':
                        uk_clean[col] = 'GBP'
                    elif col == 'collection_timestamp':
                        uk_clean[col] = datetime.now().isoformat()
                    else:
                        uk_clean[col] = None
            
            uk_clean = uk_clean[expected_columns]
            
            # Clean data types
            uk_clean['price'] = pd.to_numeric(uk_clean['price'], errors='coerce').fillna(0)
            uk_clean['year'] = pd.to_numeric(uk_clean['year'], errors='coerce').fillna(2018)
            uk_clean['mileage'] = pd.to_numeric(uk_clean['mileage'], errors='coerce')
            uk_clean['shipping_cost'] = pd.to_numeric(uk_clean['shipping_cost'], errors='coerce').fillna(0)
            uk_clean['age'] = pd.to_numeric(uk_clean['age'], errors='coerce').fillna(5)
            
            with sqlite3.connect(self.db_path) as conn:
                uk_clean.to_sql('uk_market_data', conn, if_exists='append', index=False)
                
            self.logger.info(f"âœ… Stored {len(uk_clean)} UK market records")
                
        except Exception as e:
            self.logger.error(f"Error storing UK data: {str(e)}")
    
    def store_japan_data(self, japan_data: pd.DataFrame):
        """PERMANENT store method that handles ALL Japan auction formats"""
        if japan_data.empty:
            self.logger.warning("No Japan data to store")
            return
        
        try:
            self.logger.info(f"ðŸ”„ Processing Japan data: {len(japan_data)} rows, columns: {list(japan_data.columns)}")
            
            # Use universal mapping for ALL formats
            japan_mapped = self.map_japan_columns_universal(japan_data)
            
            if japan_mapped.empty:
                self.logger.error("âŒ No data after universal mapping")
                return
            
            # Get expected database columns
            expected_columns = [
                'auction_id', 'title', 'make', 'model', 'year', 'mileage_km', 'mileage_miles',
                'final_price_jpy', 'final_price_gbp', 'auction_house', 'auction_date',
                'grade', 'grade_score', 'fuel_type', 'transmission', 'body_type', 'colour',
                'steering', 'drive_type', 'seats', 'doors', 'engine_details',
                'estimated_import_cost', 'shipping_cost', 'import_duty', 'vat', 'other_costs',
                'total_landed_cost', 'image_url', 'promo_badges', 'stock_reference', 'source', 'collection_timestamp'
            ]
            
            # Ensure all expected columns exist
            japan_clean = japan_mapped.copy()
            for col in expected_columns:
                if col not in japan_clean.columns:
                    if col == 'auction_id':
                        japan_clean[col] = [f'auto_{i}_{int(datetime.now().timestamp())}' for i in range(len(japan_clean))]
                    elif col in ['final_price_jpy', 'final_price_gbp', 'total_landed_cost']:
                        japan_clean[col] = 0.0
                    elif col == 'grade_score':
                        japan_clean[col] = 6.0
                    elif col == 'collection_timestamp':
                        japan_clean[col] = datetime.now().isoformat()
                    elif col == 'source':
                        japan_clean[col] = 'universal_upload'
                    else:
                        japan_clean[col] = None
            
            # Select only expected columns
            japan_clean = japan_clean[expected_columns]
            
            # Final data type cleaning
            numeric_columns = ['year', 'mileage_km', 'mileage_miles', 'final_price_jpy', 'final_price_gbp',
                             'grade_score', 'estimated_import_cost', 'shipping_cost', 'import_duty',
                             'vat', 'other_costs', 'total_landed_cost', 'seats', 'doors']
            
            for col in numeric_columns:
                if col in japan_clean.columns:
                    japan_clean[col] = pd.to_numeric(japan_clean[col], errors='coerce')
            
            # Fill critical missing values
            japan_clean['final_price_gbp'] = japan_clean['final_price_gbp'].fillna(0)
            japan_clean['total_landed_cost'] = japan_clean['total_landed_cost'].fillna(
                japan_clean['final_price_gbp'] * 1.35
            )
            
            # Filter valid data
            valid_rows = japan_clean[
                (japan_clean['final_price_gbp'] > 0) | 
                (japan_clean['final_price_jpy'] > 0) |
                (japan_clean['make'].notna()) |
                (japan_clean['title'].notna())
            ]
            
            if valid_rows.empty:
                self.logger.error("âŒ No valid data found after processing")
                return
            
            # Store to database
            with sqlite3.connect(self.db_path) as conn:
                valid_rows.to_sql('japan_auction_data', conn, if_exists='append', index=False)
                
            self.logger.info(f"âœ… Successfully stored {len(valid_rows)} Japan auction records")
            
        except Exception as e:
            self.logger.error(f"âŒ Error storing Japan data: {str(e)}")
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
    
    def store_analysis_results(self, profitability_results: pd.DataFrame, scores: pd.DataFrame):
        """Store analysis results"""
        if scores.empty:
            self.logger.warning("No analysis results to store")
            return
        
        try:
            expected_columns = [
                'make', 'model', 'year', 'year_range', 'uk_sample_size', 'japan_sample_size',
                'match_type', 'uk_avg_price', 'uk_median_price', 'uk_min_price', 'uk_max_price',
                'uk_price_std', 'uk_avg_mileage', 'uk_median_mileage', 'uk_listings_count',
                'japan_avg_auction_price', 'japan_median_auction_price', 'japan_min_auction_price',
                'japan_max_auction_price', 'japan_avg_import_cost', 'japan_avg_total_cost',
                'japan_median_total_cost', 'japan_avg_mileage', 'japan_avg_grade', 'japan_auctions_count',
                'gross_profit', 'profit_margin', 'profit_margin_conservative', 'roi',
                'price_volatility_uk', 'risk_score', 'market_share', 'listings_density',
                'price_trend', 'demand_score', 'avg_days_listed', 'profitability_score',
                'market_demand_score', 'risk_assessment_score', 'liquidity_score',
                'market_trends_score', 'final_score', 'final_score_percentile', 'score_grade',
                'investment_category', 'recommendation', 'overall_score', 'analysis_timestamp'
            ]
            
            # Prepare data for storage
            results_clean = scores.copy()
            
            # Ensure all expected columns exist
            for col in expected_columns:
                if col not in results_clean.columns:
                    if col == 'analysis_timestamp':
                        results_clean[col] = datetime.now().isoformat()
                    elif col in ['final_score', 'overall_score', 'profitability_score', 'market_demand_score', 
                               'risk_assessment_score', 'liquidity_score', 'market_trends_score']:
                        results_clean[col] = 50.0
                    elif col == 'score_grade':
                        results_clean[col] = 'C'
                    elif col == 'investment_category':
                        results_clean[col] = 'Moderate Opportunity'
                    elif col == 'recommendation':
                        results_clean[col] = 'Analyze Further'
                    elif col == 'match_type':
                        results_clean[col] = 'exact'
                    elif col == 'price_trend':
                        results_clean[col] = 'stable'
                    else:
                        results_clean[col] = None
            
            results_clean = results_clean[expected_columns]
            
            # Ensure numeric columns are properly typed
            numeric_columns = [col for col in expected_columns if col not in 
                             ['make', 'model', 'year_range', 'match_type', 'price_trend', 'score_grade', 
                              'investment_category', 'recommendation', 'analysis_timestamp']]
            
            for col in numeric_columns:
                if col in results_clean.columns:
                    results_clean[col] = pd.to_numeric(results_clean[col], errors='coerce')
            
            with sqlite3.connect(self.db_path) as conn:
                results_clean.to_sql('analysis_results', conn, if_exists='append', index=False)
                
            self.logger.info(f"âœ… Stored {len(results_clean)} analysis results")
                
        except Exception as e:
            self.logger.error(f"Error storing analysis results: {str(e)}")
    
    def store_gov_data(self, gov_data: pd.DataFrame):
        """Store government data"""
        if gov_data.empty:
            self.logger.warning("No government data to store")
            return
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for _, row in gov_data.iterrows():
                    data_content = json.dumps(row.to_dict())
                    
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO government_data (data_type, data_content, collection_timestamp)
                        VALUES (?, ?, ?)
                    ''', ('registration_stats', data_content, datetime.now().isoformat()))
                
                conn.commit()
                self.logger.info(f"âœ… Stored {len(gov_data)} government data records")
                
        except Exception as e:
            self.logger.error(f"Error storing government data: {str(e)}")
    
    def get_uk_data(self, days_back: int = 7) -> pd.DataFrame:
        """Get UK market data from last N days"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days_back)).isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                query = '''
                    SELECT * FROM uk_market_data 
                    WHERE collection_timestamp >= ?
                    ORDER BY collection_timestamp DESC
                '''
                
                df = pd.read_sql_query(query, conn, params=(cutoff_date,))
                self.logger.info(f"Retrieved {len(df)} UK market records")
                return df
                
        except Exception as e:
            self.logger.error(f"Error retrieving UK data: {str(e)}")
            return pd.DataFrame()
    
    def get_japan_data(self, days_back: int = 7) -> pd.DataFrame:
        """Get Japan auction data from last N days"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days_back)).isoformat()
            
            with sqlite3.connect(self.db_path) as conn:
                query = '''
                    SELECT * FROM japan_auction_data 
                    WHERE collection_timestamp >= ?
                    ORDER BY collection_timestamp DESC
                '''
                
                df = pd.read_sql_query(query, conn, params=(cutoff_date,))
                self.logger.info(f"Retrieved {len(df)} Japan auction records")
                return df
                
        except Exception as e:
            self.logger.error(f"Error retrieving Japan data: {str(e)}")
            return pd.DataFrame()
    
    def get_latest_analysis_results(self, limit: int = 100) -> pd.DataFrame:
        """Get latest analysis results"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = '''
                    SELECT * FROM analysis_results 
                    ORDER BY final_score DESC, created_at DESC
                    LIMIT ?
                '''
                
                df = pd.read_sql_query(query, conn, params=(limit,))
                self.logger.info(f"Retrieved {len(df)} analysis results")
                return df
                
        except Exception as e:
            self.logger.error(f"Error retrieving analysis results: {str(e)}")
            return pd.DataFrame()
    
    def get_market_summary(self) -> Dict:
        """Get market summary statistics"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # UK market summary
                cursor.execute('''
                    SELECT 
                        COUNT(*) as total_listings,
                        AVG(price) as avg_price,
                        MIN(price) as min_price,
                        MAX(price) as max_price,
                        COUNT(DISTINCT make) as unique_makes
                    FROM uk_market_data 
                    WHERE collection_timestamp >= ?
                ''', ((datetime.now() - timedelta(days=7)).isoformat(),))
                
                uk_summary = cursor.fetchone()
                
                # Japan auction summary
                cursor.execute('''
                    SELECT 
                        COUNT(*) as total_auctions,
                        AVG(final_price_gbp) as avg_price,
                        MIN(final_price_gbp) as min_price,
                        MAX(final_price_gbp) as max_price,
                        AVG(total_landed_cost) as avg_total_cost
                    FROM japan_auction_data 
                    WHERE collection_timestamp >= ?
                ''', ((datetime.now() - timedelta(days=7)).isoformat(),))
                
                japan_summary = cursor.fetchone()
                
                # Analysis summary
                cursor.execute('''
                    SELECT 
                        COUNT(*) as total_analyzed,
                        AVG(profit_margin) as avg_profit_margin,
                        MAX(profit_margin) as max_profit_margin,
                        AVG(final_score) as avg_score,
                        MAX(final_score) as max_score,
                        SUM(gross_profit) as total_profit_potential
                    FROM analysis_results 
                    WHERE analysis_timestamp >= ?
                ''', ((datetime.now() - timedelta(days=7)).isoformat(),))
                
                analysis_summary = cursor.fetchone()
                
                return {
                    'uk_market': {
                        'total_listings': uk_summary[0] if uk_summary else 0,
                        'avg_price': uk_summary[1] if uk_summary else 0,
                        'price_range': f"Â£{uk_summary[2]:,.0f} - Â£{uk_summary[3]:,.0f}" if uk_summary and uk_summary[2] else "N/A",
                        'unique_makes': uk_summary[4] if uk_summary else 0
                    },
                    'japan_market': {
                        'total_auctions': japan_summary[0] if japan_summary else 0,
                        'avg_auction_price': japan_summary[1] if japan_summary else 0,
                        'avg_total_cost': japan_summary[4] if japan_summary else 0,
                        'price_range': f"Â£{japan_summary[2]:,.0f} - Â£{japan_summary[3]:,.0f}" if japan_summary and japan_summary[2] else "N/A"
                    },
                    'analysis': {
                        'vehicles_analyzed': analysis_summary[0] if analysis_summary else 0,
                        'avg_profit_margin': analysis_summary[1] if analysis_summary and analysis_summary[1] else 0,
                        'max_profit_margin': analysis_summary[2] if analysis_summary and analysis_summary[2] else 0,
                        'avg_score': round(analysis_summary[3], 1) if analysis_summary and analysis_summary[3] else 0,
                        'max_score': round(analysis_summary[4], 1) if analysis_summary and analysis_summary[4] else 0,
                        'total_profit_potential': analysis_summary[5] if analysis_summary and analysis_summary[5] else 0
                    },
                    'last_updated': datetime.now().isoformat()
                }
                
        except Exception as e:
            self.logger.error(f"Error generating market summary: {str(e)}")
            return {}
    
    def get_database_stats(self) -> Dict:
        """Get database statistics"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                tables = ['uk_market_data', 'japan_auction_data', 'government_data', 'analysis_results']
                stats = {}
                
                for table in tables:
                    try:
                        cursor.execute(f'SELECT COUNT(*) FROM {table}')
                        count = cursor.fetchone()[0]
                        stats[table] = count
                    except:
                        stats[table] = 0
                
                # Database file size
                db_size = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
                stats['database_size_mb'] = round(db_size / (1024 * 1024), 2)
                
                return stats
                
        except Exception as e:
            self.logger.error(f"Error getting database stats: {str(e)}")
            return {}
    
    def cleanup_old_data(self, days_to_keep: int = 30):
        """Clean up old data beyond retention period"""
        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute('DELETE FROM uk_market_data WHERE collection_timestamp < ?', (cutoff_date,))
                uk_deleted = cursor.rowcount
                
                cursor.execute('DELETE FROM japan_auction_data WHERE collection_timestamp < ?', (cutoff_date,))
                japan_deleted = cursor.rowcount
                
                cursor.execute('DELETE FROM analysis_results WHERE analysis_timestamp < ?', (cutoff_date,))
                analysis_deleted = cursor.rowcount
                
                conn.commit()
                
                self.logger.info(f"âœ… Cleanup completed: UK:{uk_deleted}, Japan:{japan_deleted}, Analysis:{analysis_deleted}")
                
        except Exception as e:
            self.logger.error(f"Error during cleanup: {str(e)}")
    
    def validate_csv_format(self, df: pd.DataFrame, data_type: str) -> Dict:
        """Universal validation for all CSV formats"""
        validation_result = {
            'valid': False,
            'message': '',
            'suggestions': [],
            'column_mapping': {},
            'detected_format': None
        }
        
        if df.empty:
            validation_result['message'] = 'File is empty'
            return validation_result
        
        if data_type == 'japan':
            # Check for various Japan auction data indicators
            format_indicators = {
                'bmw_new_format': ['Car Title', 'Year', 'Engine', 'Mileage', 'Price'],
                'bmw_old_format': ['Car Title', 'Car Year', 'Car Engine', 'Car Mileage', 'Car Price'],
                'standard_japan': ['make', 'model', 'price', 'year'],
                'auction_format': ['auction_id', 'final_price', 'grade', 'auction_house']
            }
            
            detected_format = None
            max_matches = 0
            
            for format_name, indicators in format_indicators.items():
                matches = sum(1 for indicator in indicators 
                            if any(indicator.lower() in col.lower() for col in df.columns))
                
                if matches > max_matches:
                    max_matches = matches
                    detected_format = format_name
            
            if max_matches >= 3:  # At least 3 indicators found
                validation_result['valid'] = True
                validation_result['detected_format'] = detected_format
                validation_result['message'] = f'Japan auction data detected ({detected_format}) with {len(df)} rows'
                
                # Test mapping
                mapped_df = self.map_japan_columns_universal(df)
                validation_result['column_mapping'] = {
                    'original_columns': list(df.columns),
                    'mapped_columns': list(mapped_df.columns),
                    'mapping_success': True,
                    'format_type': detected_format
                }
            else:
                validation_result['message'] = 'Japan auction data format not recognized'
                validation_result['suggestions'] = [
                    'Include vehicle identification columns (title, make, model)',
                    'Include price columns (price, final_price, etc.)',
                    'Include year information',
                    f'Your columns: {", ".join(list(df.columns)[:10])}'
                ]
        
        elif data_type == 'uk':
            # UK format validation
            uk_indicators = [
                ['price', 'cost', 'gbp', 'pound'],
                ['make', 'brand', 'manufacturer'],
                ['title', 'description', 'name']
            ]
            
            found_indicators = []
            for indicator_group in uk_indicators:
                found = False
                for indicator in indicator_group:
                    if any(indicator.lower() in col.lower() for col in df.columns):
                        found = True
                        break
                found_indicators.append(found)
            
            if sum(found_indicators) >= 2:
                validation_result['valid'] = True
                validation_result['message'] = f'UK market data format detected with {len(df)} rows'
                validation_result['detected_format'] = 'uk_market_format'
            else:
                validation_result['message'] = 'UK market data format not recognized'
                validation_result['suggestions'] = [
                    'Include price column in GBP',
                    'Include vehicle make column',
                    'Include title or description column',
                    f'Your columns: {", ".join(list(df.columns)[:10])}'
                ]
        
        return validation_result
    
    def test_japan_upload(self, df: pd.DataFrame) -> Dict:
        """Test Japan data upload without storing"""
        
        try:
            self.logger.info(f"ðŸ§ª Testing Japan upload with {len(df)} rows")
            self.logger.info(f"Input columns: {list(df.columns)}")
            
            # Validate format
            validation_result = self.validate_csv_format(df, 'japan')
            
            if not validation_result['valid']:
                return {
                    'success': False,
                    'error': validation_result['message'],
                    'suggestions': validation_result['suggestions'],
                    'input_columns': list(df.columns)
                }
            
            # Test the universal mapping
            mapped_df = self.map_japan_columns_universal(df)
            self.logger.info(f"Mapped to {len(mapped_df.columns)} columns")
            
            # Check data quality
            has_price = False
            has_make = False
            has_title = False
            
            if 'final_price_gbp' in mapped_df.columns:
                price_values = pd.to_numeric(mapped_df['final_price_gbp'], errors='coerce')
                has_price = (price_values > 0).any()
            
            if 'make' in mapped_df.columns:
                has_make = mapped_df['make'].notna().any()
            
            if 'title' in mapped_df.columns:
                has_title = mapped_df['title'].notna().any()
            
            # Count potentially valid rows
            valid_rows = mapped_df[
                (pd.to_numeric(mapped_df.get('final_price_gbp', []), errors='coerce') > 0) | 
                (pd.to_numeric(mapped_df.get('final_price_jpy', []), errors='coerce') > 0) |
                (mapped_df.get('make', pd.Series()).notna()) |
                (mapped_df.get('title', pd.Series()).notna())
            ]
            
            test_result = {
                'success': len(valid_rows) > 0,
                'valid_rows': len(valid_rows),
                'total_rows': len(df),
                'validation_result': validation_result,
                'data_quality': {
                    'has_price_data': has_price,
                    'has_make_data': has_make,
                    'has_title_data': has_title
                },
                'sample_mapped_data': mapped_df.head(3).to_dict('records') if len(mapped_df) > 0 else [],
                'column_mapping': {
                    'input_columns': list(df.columns),
                    'output_columns': list(mapped_df.columns)
                },
                'issues': []
            }
            
            # Add specific feedback
            if not has_price:
                test_result['issues'].append('No valid price data found')
            
            if not has_make and not has_title:
                test_result['issues'].append('No vehicle identification data found')
            
            if len(valid_rows) == 0:
                test_result['issues'].append('No rows contain sufficient data for import')
            
            self.logger.info(f"ðŸ§ª Test result: {test_result['success']}, valid rows: {len(valid_rows)}/{len(df)}")
            
            return test_result
            
        except Exception as e:
            self.logger.error(f"Error testing Japan upload: {str(e)}")
            return {
                'success': False,
                'error': f'Processing error: {str(e)}',
                'input_columns': list(df.columns) if not df.empty else [],
                'issues': [f'System error: {str(e)}']
            }
    
    def generate_japan_template_csv(self) -> pd.DataFrame:
        """Generate template CSV for Japan auction data"""
        template_data = {
            'Car Title': ['2018 BMW 1 Series 118i', '2019 Toyota Prius Hybrid', '2017 Honda Civic Type R'],
            'Year': ['2018', '2019', '2017'],
            'Engine': ['1500cc', '1800cc', '2000cc'],
            'Mileage': ['13777 km', '25000 km', '35000 km'],
            'Transmission': ['AT', 'CVT', 'MT'],
            'Model Code': ['DBA-1R15', 'ZVW50', 'FK8'],
            'Fuel': ['Petrol', 'Hybrid', 'Petrol'],
            'Color': ['White', 'Silver', 'Blue'],
            'Steering': ['RHD', 'RHD', 'RHD'],
            'Drive Type': ['2WD', 'FF', 'FF'],
            'Seats': ['5', '5', '4'],
            'Doors': ['5', '5', '5'],
            'Stock ID': ['sat-44758442', 'uss-12345', 'taa-67890'],
            'Location': ['Aichi', 'Tokyo', 'Osaka'],
            'Price': ['$10,976', '$8,500', '$12,300']
        }
        
        return pd.DataFrame(template_data)
    
    def generate_uk_template_csv(self) -> pd.DataFrame:
        """Generate template CSV for UK market data"""
        template_data = {
            'item_id': ['uk_001', 'uk_002', 'uk_003'],
            'title': ['2018 BMW 1 Series 118i Sport', '2019 Toyota Prius Excel CVT', '2017 Honda Civic Type R'],
            'make': ['BMW', 'Toyota', 'Honda'],
            'model': ['1 Series', 'Prius', 'Civic'],
            'year': [2018, 2019, 2017],
            'price': [15500, 18900, 22500],
            'mileage': [45000, 28000, 38000],
            'fuel_type': ['Petrol', 'Hybrid', 'Petrol'],
            'transmission': ['Automatic', 'CVT', 'Manual'],
            'location': ['London', 'Manchester', 'Birmingham']
        }
        
        return pd.DataFrame(template_data)